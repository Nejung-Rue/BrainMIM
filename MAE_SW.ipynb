{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'server' # 'local' or 'server'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import copy, tqdm\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "# dead kernel 방지\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# GPU setting\n",
    "if env == 'server':\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # Arrange GPU devices starting from 0\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # Set the GPU 2 to use\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# python에서 List, Dict, Tuple, Set와 같은 파이썬 내장 자료구조에 대한 타입을 명시해야할 때 사용\n",
    "from typing import Sequence, Union\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    ")\n",
    "from monai.transforms import MapTransform\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    ScaleIntensityd,\n",
    "    NormalizeIntensityd,\n",
    "    Resized,\n",
    "    SpatialPadd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandSpatialCropSamplesd,\n",
    "    CenterSpatialCropd,\n",
    "    CropForegroundd,\n",
    "    RandAffined,\n",
    "    EnsureTyped,\n",
    ")\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "# PyTorch torch.load 함수를 기반으로 동작하며, 딕셔너리에 모델의 가중치, epoch 정보 등을 저장하여 모델을 재구성함.\n",
    "from timm.models.layers import trunc_normal_ # timm: pretrained model 제공, trunc_normal_: model initialize할 때 사용\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning 안뜨도록\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Palette setting\n",
    "import seaborn as sns\n",
    "sns.set_palette('Pastel1')\n",
    "palette1 = sns.color_palette('Pastel1', 8) # 5: 팔레트 몇개 생성할건지\n",
    "palette2 = sns.color_palette('Pastel2', 8) # 5: 팔레트 몇개 생성할건지\n",
    "# sns.palplot(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = ['flair', 't1', 't1ce', 't2','seg'] # 'flair', 't1', 't1ce', 't2', 'seg'\n",
    "mo_img = ['flair', 't1', 't1ce', 't2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 4e-4 # 5e-3\n",
    "weight_decay = 5e-5\n",
    "val_interval = 1\n",
    "epochs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_str = \"{:.0e}\".format(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data directory\n",
    "- 실험 결과 저장할 디렉토리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make working directory\n",
    "from datetime import datetime\n",
    "\n",
    "expd = datetime.today().strftime(\"%Y%m%d\")+\"_\"+f\"ep{epochs}_lr{lr_str}\"\n",
    "path00 = './model_MAE2D'\n",
    "\n",
    "root_dir = os.path.join(path00,f'{expd}')\n",
    "if os.path.isdir(root_dir)==0: # 해당 주소의 폴더가 없으면 만들어줌.\n",
    "    os.mkdir(root_dir)\n",
    "    print(f\"Success in making {expd}~!\")\n",
    "else:\n",
    "    if os.listdir(root_dir):\n",
    "        raise UserWarning(f\"'{expd}' is already exist and not empty.\")\n",
    "    else:\n",
    "        print(f\"[WARNING] {expd} is already exist but empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dataset\n",
    "\n",
    "- split json 준비\n",
    "- Transform 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env == 'local':\n",
    "    data_dir = '../../Datasets/Dataset002_BRATS2017/'\n",
    "elif env == 'server':\n",
    "    data_dir = '/store8/njrue/Datasets/Dataset002_BRATS2017/'\n",
    "split_json = 'BraTS2017_ipiu.json'\n",
    "\n",
    "datasets = data_dir+split_json\n",
    "train_files = load_decathlon_datalist(datasets, is_segmentation=False, data_list_key=\"training\")\n",
    "val_files = load_decathlon_datalist(datasets, is_segmentation=False, data_list_key=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 2 is the peritumoral edema (+roi3)\n",
    "    label 4 is the GD-enhancing tumor (+roi2)\n",
    "    label 1 is the necrotic tumor core (roi1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "\n",
    "            # 연구실 논문에서 규정한 class\n",
    "            # roi1\n",
    "            result.append(d[key] == 1)\n",
    "            # # roi2\n",
    "            # result.append(np.logical_or(d[key] == 4, d[key] == 1))\n",
    "            # # roi3\n",
    "            # result.append(\n",
    "            #     np.logical_or(np.logical_or(d[key] == 1, d[key] == 2), d[key] == 4)\n",
    "            # )\n",
    "\n",
    "            d[\"seg\"] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_files).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    LoadImaged(\n",
    "        keys=modality\n",
    "    ),\n",
    "    EnsureChannelFirstd(\n",
    "        keys=mo_img\n",
    "    ),\n",
    "    # Orientationd(keys=modality, axcodes=\"RAS\"),\n",
    "    ConvertToMultiChannelBasedOnBratsClassesd(keys=['seg']),\n",
    "    # ScaleIntensityd(keys=mo_img),\n",
    "    NormalizeIntensityd(\n",
    "        keys=mo_img, \n",
    "        nonzero=True, \n",
    "        channel_wise=True\n",
    "    ),\n",
    "    EnsureTyped(\n",
    "        keys=modality\n",
    "    ),\n",
    "    CropForegroundd(keys=modality, source_key=mo_img[0]),\n",
    "    SpatialPadd(keys=modality, spatial_size=[128,128,128]), # spatial size보다 input 이미지가 크면 padding 안함.\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=modality,\n",
    "        label_key=\"seg\",\n",
    "        spatial_size=[128,128,128],\n",
    "        pos=1,\n",
    "        neg=1,\n",
    "        num_samples=16,\n",
    "    ),\n",
    "    # RandSpatialCropSamplesd(\n",
    "    #     keys=modality,\n",
    "    #     roi_size=[128,128,128],\n",
    "    #     random_size=False,\n",
    "    #     num_samples=2,\n",
    "    # ),\n",
    "])\n",
    "\n",
    "# transforms_noCrop = Compose([\n",
    "#     LoadImaged(\n",
    "#         keys=modality\n",
    "#     ),\n",
    "#     EnsureChannelFirstd(\n",
    "#         keys=mo_img\n",
    "#     ),\n",
    "#     Orientationd(keys=modality, axcodes=\"RAS\"),\n",
    "#     # ConvertToMultiChannelBasedOnBratsClassesd(keys=['seg']),\n",
    "#     # ScaleIntensityd(keys=mo_img),\n",
    "#     NormalizeIntensityd(\n",
    "#         keys=mo_img, \n",
    "#         nonzero=True, \n",
    "#         channel_wise=True\n",
    "#     ),\n",
    "#     EnsureTyped(\n",
    "#         keys=modality\n",
    "#     ),\n",
    "#     CenterSpatialCropd(\n",
    "#         keys=modality, roi_size=[240,240,128],\n",
    "#     ),\n",
    "#     CropForegroundd(keys=modality, source_key=mo_img[0]),\n",
    "#     Resized(\n",
    "#         keys=modality,\n",
    "#         spatial_size=(128,128,128),\n",
    "#         mode=['area','area','area','area',]\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "val_transforms = copy.deepcopy(train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ds = CacheDataset(data=train_files[50:60], transform=train_transforms,\n",
    "                        cache_num=5,\n",
    "                        cache_rate=1.0,\n",
    "                        num_workers=4,)\n",
    "check_loader = DataLoader(check_ds, batch_size=1, shuffle=True, num_workers=0, pin_memory=False,\n",
    "                          collate_fn=pad_list_data_collate,)\n",
    "check_data = first(check_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**Original Image shape: (1, 1, 240, 240, 155), Segmentation shape: (1, 3, 240, 240, 155)\")\n",
    "print(f\"\\t   Image shape: {np.shape(check_data['t1'])}, Segmentation shape: {np.shape(check_data['seg'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1\n",
    "flair = check_data['flair'][sample][0].permute(1,0,2)\n",
    "t1 = check_data['t1'][sample][0].permute(1,0,2)\n",
    "t1ce = check_data['t1ce'][sample][0].permute(1,0,2)\n",
    "t2 = check_data['t2'][sample][0].permute(1,0,2)\n",
    "seg = check_data['seg'][sample].permute(0,2,1,3)\n",
    "\n",
    "print(check_data['flair'].meta['filename_or_obj'],end='\\n\\n')\n",
    "print(f\"flair image shape: {flair.shape}, \"\n",
    "      f\"t1 image shape: {t1.shape}, \\n\"\n",
    "      f\"t1ce image shape:  {t1ce.shape}, \"\n",
    "      f\"t2 image shape: {t2.shape}, \\n\"\n",
    "      f\"seg image shape:   {seg.shape}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = 0\n",
    "plt.figure(\"check\", (20, 6))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.title(\"flair\")\n",
    "# plt.imshow(check_ds[3][0]['flair'][0].permute(1,0,2)[:, :, zn], cmap=\"gray\")\n",
    "plt.imshow(flair[:, :, zn], cmap=\"gray\")\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.title(\"t1\")\n",
    "plt.imshow(t1[:, :, zn], cmap=\"gray\")\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.title(\"t1ce\")\n",
    "plt.imshow(t1ce[:, :, zn], cmap=\"gray\")\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.title(\"t2\")\n",
    "plt.imshow(t2[:, :, zn], cmap=\"gray\")\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.title(\"seg\")\n",
    "# plt.imshow(seg[0,:, :, zn])\n",
    "plt.imshow(torch.sum(seg[:,:, :, zn],axis=0),vmin=0,vmax=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(data=train_files, transform=train_transforms,\n",
    "                        cache_rate=1.0,\n",
    "                        num_workers=8,)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0, pin_memory=True,\n",
    "                          collate_fn=pad_list_data_collate,)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms,\n",
    "                        cache_rate=1.0,\n",
    "                        num_workers=8,)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=0, pin_memory=True,\n",
    "                        collate_fn=pad_list_data_collate,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_train = first(train_loader)\n",
    "print(\"**Original Image shape: (1, 1, 240, 240, 155), Segmentation shape: (1, 3, 240, 240, 155)\")\n",
    "print(f\"\\t   Image shape: {np.shape(check_train['t1'])}, Segmentation shape: {np.shape(check_train['seg'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(check_train['t1ce'][5][0].permute(1,0,2)[:, :, 0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = modality\n",
    "if 'seg' in data_keys:\n",
    "    in_channels = len(data_keys)+2\n",
    "else:\n",
    "    in_channels = len(data_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.blocks.transformerblock import TransformerBlock\n",
    "from monai.networks.nets import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        patch_size: Union[Sequence[int], int],\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "        decoder_dim: int = 768,\n",
    "        decoder_depth: int = 1,\n",
    "        decoder_heads: int = 8,\n",
    "        masking_ratio: float = 0.75,\n",
    "        revise_keys=[(\"model.\", \"\")],\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "\n",
    "        self.encoder = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "\n",
    "        # patch embedding block\n",
    "        patch_embedding = self.encoder.patch_embedding\n",
    "        self.to_patch, self.patch_to_emb = patch_embedding.patch_embeddings\n",
    "        n_patches = patch_embedding.n_patches\n",
    "        patch_dim = patch_embedding.patch_dim\n",
    "\n",
    "        # connect encoder and decoder if mismatch dimension\n",
    "        self.enc_to_dec = (\n",
    "            nn.Linear(hidden_size, decoder_dim)\n",
    "            if hidden_size != decoder_dim\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        # build up decoder transformer blocks\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    decoder_dim, decoder_dim * 4, decoder_heads, dropout_rate\n",
    "                )\n",
    "                for i in range(decoder_depth)\n",
    "            ]\n",
    "        )\n",
    "        self.decoder_norm = nn.LayerNorm(decoder_dim)\n",
    "        self.masking_ratio = masking_ratio\n",
    "        assert (\n",
    "            masking_ratio > 0 and masking_ratio < 1\n",
    "        ), \"masking ratio must be kept between 0 and 1\"\n",
    "        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n",
    "        self.decoder_pos_emb = nn.Embedding(n_patches, decoder_dim)\n",
    "\n",
    "        # embeddings to pixels\n",
    "        self.to_pixels = nn.Linear(decoder_dim, patch_dim)\n",
    "\n",
    "        self.init_weights(revise_keys=revise_keys)\n",
    "\n",
    "    def init_weights(self, pretrained=None, revise_keys=[]):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        # get patches\n",
    "        patches = self.to_patch(x)\n",
    "        batch, n_patches, *_ = patches.shape\n",
    "\n",
    "        # patch to encoder tokens and add positions\n",
    "        tokens = self.patch_to_emb(patches)\n",
    "        tokens = tokens + self.encoder.patch_embedding.position_embeddings\n",
    "\n",
    "        # calculate of patches needed to be masked, and get random indices\n",
    "        num_masked = int(self.masking_ratio * n_patches)\n",
    "        rand_indices = torch.rand(batch, n_patches, device=device).argsort(dim=-1)\n",
    "        masked_indices, unmasked_indices = (\n",
    "            rand_indices[:, :num_masked],\n",
    "            rand_indices[:, num_masked:],\n",
    "        )\n",
    "\n",
    "        # get the unmasked tokens to be encoded\n",
    "        batch_range = torch.arange(batch, device=device)[:, None]\n",
    "        tokens = tokens[batch_range, unmasked_indices]\n",
    "\n",
    "        # get the patches to be masked for the final reconstruction loss\n",
    "        # masked_patches = patches[batch_range, masked_indices]\n",
    "\n",
    "        for blk in self.encoder.blocks:\n",
    "            tokens = blk(tokens)\n",
    "        encoded_tokens = tokens \n",
    "\n",
    "        decoder_tokens = self.enc_to_dec(encoded_tokens)\n",
    "        decoder_tokens += self.decoder_pos_emb(unmasked_indices)\n",
    "\n",
    "        mask_tokens = repeat(self.mask_token, \"d -> b n d\", b=batch, n=num_masked)\n",
    "        mask_tokens = mask_tokens + self.decoder_pos_emb(masked_indices)\n",
    "\n",
    "        # concat the masked tokens to the decoder tokens and attend with decoder\n",
    "        decoder_tokens = torch.cat((mask_tokens, decoder_tokens), dim=1)\n",
    "        for blk in self.decoder_blocks:\n",
    "            decoder_tokens = blk(decoder_tokens)\n",
    "        decoded_tokens = self.decoder_norm(decoder_tokens)\n",
    "\n",
    "        # splice out the mask tokens and project to pixel values\n",
    "        mask_tokens = decoded_tokens[:, :num_masked]\n",
    "        pred_pixel_values = self.to_pixels(mask_tokens)\n",
    "\n",
    "        return pred_pixel_values, patches, batch_range, masked_indices, unmasked_indices, encoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAE(\n",
    "    in_channels=len(mo_img),\n",
    "    img_size=[128,128,128],\n",
    "    patch_size=[16,16,16],\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    pos_embed='perceptron',\n",
    "    dropout_rate=0.0,\n",
    "    spatial_dims=3,\n",
    "    masking_ratio=0.75,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model_MAE2D/20231221_load1220_ep1000_lr1e-04/'\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"best_model.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = nn.L1Loss()\n",
    "# recon_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "best_loss = 100\n",
    "best_loss_epoch = -1\n",
    "epoch_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "real_labels = [] # validation dataset의 label을 모을 list\n",
    "pred_labels = [] # best model의 validation dataset에 대한 prediction을 모을 list\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        flairs, t1s, t1ces, t2s = batch_data['flair'].to(device), batch_data['t1'].to(device), batch_data['t1ce'].to(device), \\\n",
    "                                        batch_data['t2'].to(device)\n",
    "        # t1ces = batch_data['t1ce'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_pixel_values, patches, batch_range, masked_indices, unmasked_indices, encoded_tokens = \\\n",
    "            model(torch.cat((flairs, t1s, t1ces, t2s),dim=1))\n",
    "        loss = recon_loss(pred_pixel_values, patches[batch_range, masked_indices])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        # print(f\"{step:02}/{epoch_len:02}, train_loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0 or epoch + 1 == epochs:\n",
    "        model.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        for val_data in val_loader:\n",
    "            val_flairs, val_t1s, val_t1ces, val_t2s = val_data['flair'].to(device), val_data['t1'].to(device),\\\n",
    "                                                                val_data['t1ce'].to(device),  val_data['t2'].to(device)\n",
    "            # val_t1ces = val_data['t1ce'].to(device)\n",
    "            with torch.no_grad():\n",
    "                val_pred_pixel_values, val_patches, val_batch_range, val_masked_indices, _, _ = \\\n",
    "                    model(torch.cat((val_flairs, val_t1s, val_t1ces, val_t2s),dim=1))\n",
    "                val_loss += recon_loss(pred_pixel_values, patches[batch_range, masked_indices]).item()\n",
    "                batch_size = pred_pixel_values.shape[0]\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        for _ in range(val_interval):\n",
    "            val_loss_values.append(val_loss)\n",
    "        df = pd.DataFrame({'epoch':range(len(epoch_loss_values)),'train_loss':epoch_loss_values,\n",
    "                           'val_loss':val_loss_values})\n",
    "        df = df.set_index('epoch')\n",
    "        df.to_csv(os.path.join(root_dir,'save_results.csv'))\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_loss_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), os.path.join(root_dir,\"best_model.pth\"))\n",
    "            print(\"saved new best metric model\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), os.path.join(root_dir,f\"save_model_ep{epoch+1}.pth\"))\n",
    "\n",
    "        print(f\"Current epoch: {epoch+1}, average loss: {val_loss:.4f}\")\n",
    "        print(f\"Minimum loss: {best_loss:.4f} at epoch {best_loss_epoch}\")\n",
    "\n",
    "\n",
    "print(f\"Training completed, best_loss: {best_loss:.4f} at epoch: {best_loss_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(root_dir,f\"last_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "sec = end - start\n",
    "print(f\"Training time: {str(timedelta(seconds=sec)).split('.')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(os.path.join(root_dir,\"Training_time.txt\"),\"w\")\n",
    "f.write(f\"Training time: {str(timedelta(seconds=sec)).split('.')[0]}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(\"train/valid\", (13, 5))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.title('Iteration Train Loss')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(epoch_loss_values, label='train loss')\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = range(len(val_loss_values))\n",
    "y = epoch_loss_values[:len(val_loss_values)]\n",
    "y2 = val_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y2, label='validation loss', color=palette1[1])\n",
    "plt.plot(x, y, label='train loss', color=palette1[0])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(root_dir,f'MAE_loss_graph.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display reconstruction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = './model_MAE/20231205_load0404_1500_lr8e-04/'\n",
    "# expd = 'load1204_4000_lr8e-04'\n",
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_model.pth\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title='', vmin=-4, vmax=4):\n",
    "    # image is [H, W, 3]\n",
    "    # assert image.shape[2] == 3\n",
    "    plt.imshow(image, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def run_one_image(img, model, modality='flair', expd='', slice=50, save_dir=root_dir, dim=3):\n",
    "    if dim == 3:\n",
    "        dim_info = {'h':8, 'w':8, 'd':8, 'p1':16, 'p2':16, 'p3':16, 'c':len(mo_img)}\n",
    "    elif dim == 2:\n",
    "        dim_info = {'h':8, 'w':8, 'd':1, 'p1':16, 'p2':16, 'p3':1, 'c':len(mo_img)}\n",
    "    modality_dict = {'flair':0, 't1':1, 't1ce':2, 't2':3}\n",
    "    modality_dim = modality_dict[modality]\n",
    "    \n",
    "    flairs, t1s, t1ces, t2s = img['flair'].to(device), img['t1'].to(device), \\\n",
    "        img['t1ce'].to(device), img['t2'].to(device)\n",
    "    # t1ces = img['t1ce'].to(device)\n",
    "    x = torch.cat((flairs, t1s, t1ces, t2s),dim=1)\n",
    "    # x = t1ces\n",
    "    # modality_dim = 0\n",
    "    vmin, vmax = torch.tensor(np.min(x[0,modality_dim,:,:,slice])), torch.tensor(np.max(x[0,modality_dim,:,:,slice]))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # run MAE\n",
    "        pred_pixel_values, patches, batch_range, masked_indices, unmasked_indices, encoded_tokens = model(x)\n",
    "        \n",
    "        # masked image\n",
    "        im_masked = copy.deepcopy(patches.cpu())\n",
    "        im_masked[batch_range, masked_indices] = vmin\n",
    "        im_masked = rearrange(im_masked, 'b (h w d) (p1 p2 p3 c)->b c (h p1) (w p2) (d p3)', **dim_info)\n",
    "        \n",
    "        # only reconstruction image\n",
    "        y = copy.deepcopy(patches.cpu())\n",
    "        y[batch_range, masked_indices] = pred_pixel_values.cpu()\n",
    "        y[batch_range, unmasked_indices] = vmin\n",
    "        y = rearrange(y, 'b (h w d) (p1 p2 p3 c)->b c (h p1) (w p2) (d p3)', **dim_info)\n",
    "        \n",
    "        # MAE reconstruction pasted with visible patches\n",
    "        im_paste = copy.deepcopy(patches.cpu())\n",
    "        im_paste[batch_range, masked_indices] = pred_pixel_values.cpu()\n",
    "        # im_paste[batch_range, unmasked_indices] = model.to_pixels(model.decoder_norm(encoded_tokens)).cpu()\n",
    "        im_paste = rearrange(im_paste, 'b (h w d) (p1 p2 p3 c)->b c (h p1) (w p2) (d p3)', **dim_info)\n",
    "\n",
    "        # make the plt figure larger\n",
    "        plt.rcParams['figure.figsize'] = [12, 4]\n",
    "        plt.suptitle(f\"MAE with pixel reconstruction{expd} ({modality})\",fontsize=20)\n",
    "        plt.subplot(1, 4, 1)\n",
    "        show_image(x.cpu().permute(0,1,3,2,4)[1,modality_dim,:,:,slice], \"original\", vmin, vmax)\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        show_image(im_masked.permute(0,1,3,2,4)[1,modality_dim,:,:,slice], \"masked\", vmin, vmax)\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        show_image(y.permute(0,1,3,2,4)[1,modality_dim,:,:,slice], \"reconstruction\", vmin, vmax)\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        show_image(im_paste.permute(0,1,3,2,4)[1,modality_dim,:,:,slice], \"reconstruction + visible\", vmin, vmax)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir,f'MAE_recon_results{expd}_{modality}.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = CacheDataset(data=val_files, transform=transforms_noCrop,\n",
    "#                         cache_rate=1.0,\n",
    "#                         num_workers=8,)\n",
    "# test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True,\n",
    "#                         collate_fn=pad_list_data_collate,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expd2 = 'recon_results(change_format)'\n",
    "res_path = os.path.join(root_dir,expd2)\n",
    "if os.path.isdir(res_path)==0: # 해당 주소의 폴더가 없으면 만들어줌.\n",
    "    os.mkdir(res_path)\n",
    "    print(f\"Success in making {expd}/{expd2}~!\")\n",
    "else:\n",
    "    if os.listdir(res_path):\n",
    "        raise UserWarning(f\"'{expd}/{expd2}' is already exist and not empty.\")\n",
    "    else:\n",
    "        print(f\"[WARNING] {expd}/{expd2} is already exist but empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE with pixel reconstruction:')\n",
    "for i, data in enumerate(val_loader):\n",
    "    print(f'{i}th data: ')\n",
    "    for mo in mo_img:\n",
    "        run_one_image(data, model, modality=mo, expd=f'_test{i:02}', \n",
    "                      slice=0, save_dir=res_path, dim=3) # expd 넣을거면 _로 시작하도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = first(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE with pixel reconstruction:')\n",
    "for mo in mo_img:\n",
    "    run_one_image(img, model, modality=mo, expd='_train_data', slice=0, dim=3) # expd 넣을거면 _로 시작하도록"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipiu2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
