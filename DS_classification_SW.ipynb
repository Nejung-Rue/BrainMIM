{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'server' # 'local' or 'server'\n",
    "assert env in ['loval','server'], \"Training environment must be 'local' or 'server'\"\n",
    "mode = 'MAE' # 'pyradiomics' or 'MAE' or 'ensemble'\n",
    "assert mode in ['pyradiomics','MAE','ensemble'], \"Model's mode must be 'pyradiomics' or 'MAE' or 'ensemble'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, copy\n",
    "\n",
    "# dead kernel 방지\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# GPU setting\n",
    "if env == 'server':\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # Arrange GPU devices starting from 0\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" # Set the GPU 2 to use\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging, sys\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# python에서 List, Dict, Tuple, Set와 같은 파이썬 내장 자료구조에 대한 타입을 명시해야할 때 사용\n",
    "from typing import Sequence, Union\n",
    "\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, StackDataset\n",
    "# dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "# Label encoder: Categorical label to Numerical label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# z-normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Monai\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    ")\n",
    "from monai.transforms import MapTransform\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    ScaleIntensityd,\n",
    "    NormalizeIntensityd,\n",
    "    Resized,\n",
    "    SpatialPadd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandSpatialCropSamplesd,\n",
    "    CenterSpatialCropd,\n",
    "    CropForegroundd,\n",
    "    RandAffined,\n",
    "    EnsureTyped,\n",
    ")\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "# PyTorch torch.load 함수를 기반으로 동작하며, 딕셔너리에 모델의 가중치, epoch 정보 등을 저장하여 모델을 재구성함.\n",
    "from timm.models.layers import trunc_normal_ # timm: pretrained model 제공, trunc_normal_: model initialize할 때 사용\n",
    "\n",
    "# Model performance\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, r2_score\n",
    "from scipy import stats\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning 안뜨도록\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Palette setting\n",
    "import seaborn as sns\n",
    "sns.set_palette('Pastel1')\n",
    "palette1 = sns.color_palette('Pastel1', 8) # 5: 팔레트 몇개 생성할건지\n",
    "palette = sns.color_palette('Pastel2', 8) # 5: 팔레트 몇개 생성할건지\n",
    "# sns.palplot(palette)\n",
    "\n",
    "# 한글 폰트 깨짐 해결\n",
    "import matplotlib\n",
    "if env == 'local':\n",
    "    matplotlib.rcParams['font.family'] ='Malgun Gothic'\n",
    "    matplotlib.rcParams['axes.unicode_minus'] =False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load할 모델 path\n",
    "model_dir = './model_MAE/20231205_load0404_1500_lr8e-04/last_model.pth'\n",
    "\n",
    "# pyradiomics에 shape feature를 제외할 것인지\n",
    "noShape = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "if mode == 'pyradiomics':\n",
    "    lr = 1e-5\n",
    "    weight_decay = 5e-6\n",
    "elif mode == 'ensemble':\n",
    "    lr = 1e-4\n",
    "    weight_decay = 5e-5\n",
    "else:\n",
    "    lr = 1e-4\n",
    "    weight_decay = 5e-5\n",
    "lr_str = \"{:.0e}\".format(lr)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 23\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = ['flair', 't1', 't1ce', 't2', 'seg'] # 'flair', 't1', 't1ce', 't2', 'seg'\n",
    "mo_img = ['flair', 't1', 't1ce', 't2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make working directory\n",
    "from datetime import datetime\n",
    "\n",
    "expd = datetime.today().strftime(\"%Y%m%d\")+\"_\"+f\"ep{epochs}_lr{lr_str}_05last\"\n",
    "path00 = f'./model_downstream/model_{mode}'\n",
    "\n",
    "root_dir = os.path.join(path00,f'{expd}')\n",
    "if os.path.isdir(root_dir)==0: # 해당 주소의 폴더가 없으면 만들어줌.\n",
    "    os.mkdir(root_dir)\n",
    "    print(f\"Success in making {expd}~!\")\n",
    "else:\n",
    "    if os.listdir(root_dir):\n",
    "        raise UserWarning(f\"'{expd}' is already exist and not empty.\")\n",
    "    else:\n",
    "        print(f\"[WARNING] {expd} is already exist but empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env == 'local':\n",
    "    data_dir = '../../Datasets/Dataset002_BRATS2017/'\n",
    "elif env == 'server':\n",
    "    data_dir = '/store8/njrue/Datasets/Dataset002_BRATS2017/'\n",
    "split_json = 'BraTS2017_ipiu.json'\n",
    "\n",
    "datasets = data_dir+split_json\n",
    "train_files = load_decathlon_datalist(datasets, is_segmentation=False, data_list_key=\"training\")\n",
    "val_files = load_decathlon_datalist(datasets, is_segmentation=False, data_list_key=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_files).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 2 is the peritumoral edema (+roi3)\n",
    "    label 4 is the GD-enhancing tumor (+roi2)\n",
    "    label 1 is the necrotic tumor core (roi1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "\n",
    "            # 연구실 논문에서 규정한 class\n",
    "            # roi1\n",
    "            result.append(d[key] == 1)\n",
    "            # # roi2\n",
    "            # result.append(np.logical_or(d[key] == 4, d[key] == 1))\n",
    "            # # roi3\n",
    "            # result.append(\n",
    "            #     np.logical_or(np.logical_or(d[key] == 1, d[key] == 2), d[key] == 4)\n",
    "            # )\n",
    "\n",
    "            d[\"seg\"] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    LoadImaged(\n",
    "        keys=modality\n",
    "    ),\n",
    "    EnsureChannelFirstd(\n",
    "        keys=mo_img\n",
    "    ),\n",
    "    ConvertToMultiChannelBasedOnBratsClassesd(keys=['seg']), \n",
    "    # Orientationd(keys=modality, axcodes=\"RAS\"), # 이게 seg에는 적용이 안되는 것 같음.\n",
    "    # ScaleIntensityd(keys=mo_img),\n",
    "    NormalizeIntensityd(\n",
    "        keys=mo_img, \n",
    "        nonzero=True, \n",
    "        channel_wise=True\n",
    "    ),\n",
    "    EnsureTyped(\n",
    "        keys=modality\n",
    "    ),\n",
    "    CropForegroundd(keys=modality, source_key=mo_img[0]),\n",
    "    SpatialPadd(keys=modality, spatial_size=[128,128,128]), # spatial size보다 input 이미지가 크면 padding 안함.\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=modality,\n",
    "        label_key=\"seg\",\n",
    "        spatial_size=[128,128,128],\n",
    "        pos=1,\n",
    "        neg=0,\n",
    "        num_samples=1,\n",
    "    ),\n",
    "    # RandSpatialCropSamplesd(\n",
    "    #     keys=modality,\n",
    "    #     roi_size=[128,128,128],\n",
    "    #     random_size=False,\n",
    "    #     num_samples=1,\n",
    "    # ),\n",
    "])\n",
    "\n",
    "val_transforms = copy.deepcopy(train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyradiomics\n",
    "- 최초 1회만 실행하면 됨. (전처리 결과를 새로운 csv 파일로 만들어 저장하기 때문에)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load feature & Split X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.read_csv('dataset/pyradiomics_feature.csv', sep=',')\n",
    "# features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder() # HGG=0, LGG=1\n",
    "\n",
    "# X = features.drop('target',axis=1)\n",
    "# y = pd.Series(le.fit_transform(features.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # z-normalization\n",
    "# z_norm = StandardScaler()\n",
    "\n",
    "# z_norm.fit(X)\n",
    "# X_norm = pd.DataFrame(z_norm.transform(X), columns=X.columns).fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = np.append(np.append(np.arange(16,20),np.arange(91,108)),np.arange(190,210))\n",
    "val_idx = np.append(np.append(val_idx, np.arange(218,220)),np.arange(273,285))\n",
    "train_idx = np.setdiff1d(np.arange(0,len(train_files)+len(val_files)),val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = pd.concat([X_norm, features.target], axis=1)\n",
    "# valid_dataset = new_dataset.iloc[val_idx]\n",
    "# train_dataset = new_dataset.iloc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataset.to_csv('./dataset/pyradiomics_val.csv', index=False)\n",
    "# train_dataset.to_csv('./dataset/pyradiomics_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'pyradiomics':\n",
    "    train_ds = CacheDataset(data=train_files, transform=train_transforms,\n",
    "                            cache_rate=1.0,\n",
    "                            num_workers=8,)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, \n",
    "                            collate_fn=pad_list_data_collate,)\n",
    "\n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms,\n",
    "                            cache_rate=1.0,\n",
    "                            num_workers=8,)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True,\n",
    "                            collate_fn=pad_list_data_collate,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'pyradiomics':\n",
    "    check_train = first(train_loader)\n",
    "    print(\"**Original Image shape: (1, 1, 240, 240, 155), Segmentation shape: (1, 3, 240, 240, 155)\")\n",
    "    print(f\"\\t   Image shape: {np.shape(check_train['t1'])}, Segmentation shape: {np.shape(check_train['seg'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'pyradiomics':\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(check_train['seg'][0][0].permute(1,0,2)[:, :, 64])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'pyradiomics':\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(check_train['t1ce'][0][0].permute(1,0,2)[:, :, 64], cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pyradiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('dataset/pyradiomics_train.csv', sep=',') # .sample(frac=1)\n",
    "val_dataset = pd.read_csv('dataset/pyradiomics_val.csv', sep=',') # .sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape feature가 너무 절대적이기 때문에 shape feature가 없는 버전을 만드는 중\n",
    "\n",
    "train_dataset_noShape = copy.deepcopy(train_dataset)\n",
    "val_dataset_noShape = copy.deepcopy(val_dataset)\n",
    "\n",
    "for col in train_dataset.columns:\n",
    "    if 'shape' in col:\n",
    "        train_dataset_noShape.drop(columns=col, inplace=True)\n",
    "        val_dataset_noShape.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if noShape == True:\n",
    "    train_dataset = train_dataset_noShape\n",
    "    val_dataset = val_dataset_noShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() # HGG=0, LGG=1\n",
    "\n",
    "X_train = train_dataset.drop('target',axis=1)\n",
    "y_train = pd.Series(le.fit_transform(train_dataset.target))\n",
    "X_val = val_dataset.drop('target',axis=1)\n",
    "y_val = pd.Series(le.transform(val_dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch.tensor\n",
    "X_train_t = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train_t = torch.from_numpy(y_train).to(torch.float32).reshape(-1,1)\n",
    "X_val_t = torch.from_numpy(X_val).to(torch.float32)\n",
    "y_val_t = torch.from_numpy(y_val).to(torch.float32).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "training_data = TensorDataset(X_train_t, y_train_t)\n",
    "val_data = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_py = DataLoader(training_data, batch_size=batch_size, shuffle=True,)\n",
    "val_loader_py = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in train_loader_py:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both: stack dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'ensemble':\n",
    "    # Stack two datasets\n",
    "    stack_train_ds = StackDataset(train_ds, training_data)\n",
    "    stack_val_ds = StackDataset(val_ds, val_data)\n",
    "\n",
    "    # Create Dataloader\n",
    "    stack_train_loader = DataLoader(stack_train_ds, batch_size=batch_size, shuffle=True,)\n",
    "    stack_val_loader = DataLoader(stack_val_ds, batch_size=batch_size, shuffle=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.blocks.transformerblock import TransformerBlock\n",
    "from monai.networks.nets import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT), based on: \"Dosovitskiy et al.,\n",
    "    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>\"\n",
    "\n",
    "    Modified to also give same dimension outputs as the input size of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        patch_size: Union[Sequence[int], int],\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "        masking_ratio: float = 0.0,\n",
    "        revise_keys=[(\"model.\", \"\")],\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels or the number of channels for input\n",
    "            img_size: dimension of input image.\n",
    "            patch_size: dimension of patch size.\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            mlp_dim: dimension of feedforward layer.\n",
    "            num_layers: number of transformer blocks.\n",
    "            num_heads: number of attention heads.\n",
    "            pos_embed: position embedding layer type.\n",
    "            dropout_rate: faction of the input units to drop.\n",
    "            spatial_dims: number of spatial dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.masking_ratio = masking_ratio\n",
    "\n",
    "        self.encoder = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "\n",
    "        # patch embedding block\n",
    "        patch_embedding = self.encoder.patch_embedding\n",
    "        self.to_patch, self.patch_to_emb = patch_embedding.patch_embeddings\n",
    "\n",
    "        self.MAE_outhidden = nn.Sequential(\n",
    "            # patch 별로 768개의 feature를 가지고 있는 것을 64로 축약\n",
    "            nn.Linear(in_features=768, out_features=256, bias=True), # 768 > 64\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=64, bias=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.MAE_hidden = nn.Sequential(\n",
    "            # nn.Linear(393216, 1024, bias=True), # 96일 때 165888(4*96*768), 128일 때 393216(4*128*768)\n",
    "            nn.Linear(32768, 4096, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1024, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512, bias=True), # 96일 때 165888\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256,bias=True)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 1, bias=True),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        self.init_weights(revise_keys=revise_keys)\n",
    "\n",
    "    def init_weights(self, pretrained=None, revise_keys=[]):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor must have isotropic spatial dimensions,\n",
    "                such as ``[batch_size, channels, sp_size, sp_size[, sp_size]]``.\n",
    "        \"\"\"\n",
    "\n",
    "        # get patches\n",
    "        patches = self.to_patch(x)\n",
    "\n",
    "        # patch to encoder tokens and add positions\n",
    "        tokens = self.patch_to_emb(patches)\n",
    "        \n",
    "        tokens = tokens + self.encoder.patch_embedding.position_embeddings\n",
    "        \n",
    "        for blk in self.encoder.blocks:\n",
    "            tokens = blk(tokens)\n",
    "        encoded_tokens = tokens # 이게 인코더를 거친 결과? latent space??\n",
    "        encoded_tokens = self.MAE_outhidden(encoded_tokens)\n",
    "        latent_space = self.flatten(encoded_tokens)\n",
    "\n",
    "        hidden = self.MAE_hidden(latent_space)\n",
    "        pred = self.classifier(hidden)\n",
    "\n",
    "        return pred\n",
    "\n",
    "# print(MAE_MLP(in_channels=4, img_size=[96,96,96],patch_size=[16,16,16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyradiomics MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP with pyradiomics features\n",
    "class py_MLP(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(py_MLP, self).__init__()\n",
    "\n",
    "    self.input = nn.Sequential(\n",
    "      nn.Linear(X.size(-1), 512, bias=True),\n",
    "      nn.Tanh(),\n",
    "    )\n",
    "    self.hidden = nn.Sequential(\n",
    "      nn.Linear(512, 256, bias=True),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(256, 128, bias=True), # 256 > 128\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.output = nn.Sequential(\n",
    "      nn.Linear(128, 1, bias=True),\n",
    "      # nn.Sigmoid(),\n",
    "    )\n",
    "    # torch.nn.init.xavier_uniform_(self.output.weight),\n",
    "\n",
    "  def forward(self, x):\n",
    "    pred = self.input(x)\n",
    "    hidden = self.hidden(pred)\n",
    "    pred = self.output(hidden)\n",
    "    return pred\n",
    "\n",
    "print(py_MLP())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class en_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT), based on: \"Dosovitskiy et al.,\n",
    "    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>\"\n",
    "\n",
    "    Modified to also give same dimension outputs as the input size of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        patch_size: Union[Sequence[int], int],\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"perceptron\",\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "        masking_ratio: float = 0.0,\n",
    "        revise_keys=[(\"model.\", \"\")],\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels or the number of channels for input\n",
    "            img_size: dimension of input image.\n",
    "            patch_size: dimension of patch size.\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            mlp_dim: dimension of feedforward layer.\n",
    "            num_layers: number of transformer blocks.\n",
    "            num_heads: number of attention heads.\n",
    "            pos_embed: position embedding layer type.\n",
    "            dropout_rate: faction of the input units to drop.\n",
    "            spatial_dims: number of spatial dimensions.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.masking_ratio = masking_ratio\n",
    "\n",
    "        self.encoder = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "\n",
    "        # patch embedding block\n",
    "        patch_embedding = self.encoder.patch_embedding\n",
    "        self.to_patch, self.patch_to_emb = patch_embedding.patch_embeddings\n",
    "\n",
    "        self.MAE_outhidden = nn.Sequential(\n",
    "            # patch 별로 768개의 feature를 가지고 있는 것을 64로 축약\n",
    "            nn.Linear(in_features=768, out_features=256, bias=True), # 768 > 64\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=64, bias=True),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.MAE_hidden = nn.Sequential(\n",
    "            # nn.Linear(393216, 1024, bias=True), # 96일 때 165888(4*96*768), 128일 때 393216(4*128*768)\n",
    "            nn.Linear(32768, 4096, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1024, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512, bias=True), # 96일 때 165888\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.py_hidden = nn.Sequential(\n",
    "            nn.Linear(X.size(-1),512,bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512,256,bias=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden = nn.Sequential(\n",
    "            # nn.Linear(X.size(-1)+1024,256, bias=True),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(512+256,512, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256, bias=True), # 256 -> 64\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 1, bias=True),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.init_weights(revise_keys=revise_keys)\n",
    "\n",
    "    def init_weights(self, pretrained=None, revise_keys=[]):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def forward(self, x_mae, x_py):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor must have isotropic spatial dimensions,\n",
    "                such as ``[batch_size, channels, sp_size, sp_size[, sp_size]]``.\n",
    "        \"\"\"\n",
    "\n",
    "        # get patches\n",
    "        patches = self.to_patch(x_mae)\n",
    "\n",
    "        # patch to encoder tokens and add positions\n",
    "        tokens = self.patch_to_emb(patches)\n",
    "        \n",
    "        tokens = tokens + self.encoder.patch_embedding.position_embeddings\n",
    "        \n",
    "        for blk in self.encoder.blocks:\n",
    "            tokens = blk(tokens)\n",
    "        encoded_tokens = tokens\n",
    "        encoded_tokens = self.MAE_outhidden(encoded_tokens)\n",
    "        latent_space = self.flatten(encoded_tokens)\n",
    "\n",
    "        mae_hidden = self.MAE_hidden(latent_space)\n",
    "        py_hidden = self.py_hidden(x_py)\n",
    "        \n",
    "        concat_inputs = torch.concat((mae_hidden,py_hidden), dim=1)\n",
    "        hidden = self.hidden(concat_inputs)\n",
    "        pred = self.classifier(hidden)\n",
    "\n",
    "        return pred\n",
    "\n",
    "# print(en_MLP(in_channels=4, img_size=[96,96,96],patch_size=[16,16,16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder_weight(model_dir, model):\n",
    "    state_dict = torch.load(model_dir, map_location=device)\n",
    "    \n",
    "    for block in ['encoder','to_patch','patch_to_emb']:\n",
    "        state_dict_temp = copy.deepcopy(state_dict)\n",
    "        for key in list(state_dict.keys()):\n",
    "            if key == 'encoder.patch_embedding.position_embeddings':\n",
    "                new_key = key.replace(block+'.', \"\")\n",
    "                state_dict_temp[new_key] = model.encoder.patch_embedding.position_embeddings\n",
    "                _ = state_dict_temp.pop(key)\n",
    "            elif block in key:\n",
    "                new_key = key.replace(block+'.', \"\")\n",
    "                state_dict_temp[new_key] = state_dict_temp.pop(key)\n",
    "            else:\n",
    "                _ = state_dict_temp.pop(key)\n",
    "        \n",
    "        if block=='encoder':\n",
    "            model.encoder.load_state_dict(state_dict_temp)\n",
    "        elif block=='to_patch':\n",
    "            model.to_patch.load_state_dict(state_dict_temp)\n",
    "        elif block=='patch_to_emb':\n",
    "            model.patch_to_emb.load_state_dict(state_dict_temp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode=='pyradiomics':\n",
    "    model = py_MLP().to(device)\n",
    "elif mode=='MAE':\n",
    "    model = MAE_MLP(\n",
    "        in_channels=4,\n",
    "        img_size=[128,128,128],\n",
    "        patch_size=[16,16,16],\n",
    "        hidden_size=768,\n",
    "        mlp_dim=3072,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        pos_embed='perceptron',\n",
    "        dropout_rate=0.0,\n",
    "        spatial_dims=3,\n",
    "    ).to(device)\n",
    "    model = load_encoder_weight(model_dir=model_dir, model=model)\n",
    "else:\n",
    "    model = en_MLP(\n",
    "        in_channels=4,\n",
    "        img_size=[128,128,128],\n",
    "        patch_size=[16,16,16],\n",
    "        hidden_size=768,\n",
    "        mlp_dim=3072,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        pos_embed='perceptron',\n",
    "        dropout_rate=0.0,\n",
    "        spatial_dims=3,\n",
    "    ).to(device)\n",
    "    model = load_encoder_weight(model_dir=model_dir, model=model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# Loss function\n",
    "# loss_function = nn.BCEWithLogitsLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "# CrossEntropyLoss: 다중클래스 분류에 사용하는 loss func\n",
    "\n",
    "# BCEWithLogitsLoss: Sigmoid + BCELoss (BCELoss와 Sigmoid를 따로 쓰는 것보다 더 안정적)\n",
    "# Output이 0~1 사이 값일 필요가 없음\n",
    "\n",
    "# BCELoss : Sigmoid를 모델 안에 추가하면 안되고, 모델의 출력 값에 따로 저장해줘야함. Sigmoid를 모델 안에 넣게되면 학습이 잘 안됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_MAE(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    pred_list, y_list = [], []\n",
    "    train_running_loss = 0\n",
    "    train_running_correct = 0\n",
    "    for batch_data in dataloader:\n",
    "        flairs, t1s, t1ces, t2s = batch_data['flair'].to(device), batch_data['t1'].to(device), \\\n",
    "            batch_data['t1ce'].to(device), batch_data['t2'].to(device)\n",
    "        X_mae = torch.cat((flairs, t1s, t1ces, t2s),dim=1)\n",
    "        y = (batch_data['label']).type(torch.float).reshape([-1,1]).to(device)\n",
    "        \n",
    "        # 예측 오류 계산\n",
    "        pred = model(X_mae) # forward pass\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "        y_list.extend([i.item() for i in list(y.cpu())]); pred_list.extend([i.item() for i in list(pred.cpu())])\n",
    "        loss = loss_fn(pred, y) # calculate the loss\n",
    "        train_running_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        y_pred = (pred>=0.5).type(torch.int8)\n",
    "        train_running_correct += torch.eq(y_pred, y).sum().item()\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = train_running_loss / len(dataloader)\n",
    "    epoch_acc = train_running_correct / size\n",
    "    print(f\"Train: \\n Accuracy: {epoch_acc:>0.5f}%, Avg loss: {epoch_loss:>7f}\")\n",
    "    return epoch_loss, epoch_acc, y_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_MAE(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    pred_list, y_list = [], []\n",
    "    valid_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # no training\n",
    "        for batch_data in dataloader:\n",
    "            flairs, t1s, t1ces, t2s = batch_data['flair'].to(device), batch_data['t1'].to(device), \\\n",
    "                batch_data['t1ce'].to(device), batch_data['t2'].to(device)                \n",
    "            X_mae = torch.cat((flairs, t1s, t1ces, t2s),dim=1)  \n",
    "            y = (batch_data['label']).type(torch.float).reshape([-1,1]).to(device)\n",
    "            \n",
    "            pred = model(X_mae)\n",
    "            pred = nn.Sigmoid()(pred)\n",
    "            y_list.extend([i.item() for i in list(y.cpu())]); pred_list.extend([i.item() for i in list(pred.cpu())])\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            # correct += torch.eq((pred>0).type(torch.int8), y).sum().item()\n",
    "            correct += torch.eq((pred>=0.5).type(torch.int8), y).sum().item()\n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation: \\n Accuracy: {correct:>0.5f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "\n",
    "    return valid_loss, correct, y_list, pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function: pyradiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_py(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    y_list, pred_list = [],[]\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    train_running_loss = 0\n",
    "    train_running_correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 예측 오류 계산\n",
    "        pred = model(X) # forward pass\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "        y_list.extend([i.item() for i in list(y.cpu())]); pred_list.extend([i.item() for i in list(pred.cpu())])\n",
    "        loss = loss_fn(pred, y) # calculate the loss\n",
    "        train_running_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        # y_pred = (pred>0).type(torch.int8)\n",
    "        y_pred = (pred>=0.5).type(torch.int8)\n",
    "        train_running_correct += torch.eq(y_pred, y).sum().item()\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = train_running_loss / len(dataloader)\n",
    "    epoch_acc = train_running_correct / size\n",
    "    print(f\"Train: \\n Accuracy: {epoch_acc:>0.5f}%, Avg loss: {epoch_loss:>7f}\")\n",
    "    \n",
    "    return epoch_loss, epoch_acc, y_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_py(dataloader, model, loss_fn):\n",
    "    y_list, pred_list = [],[]\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    valid_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # no training\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            pred = nn.Sigmoid()(pred)\n",
    "            y_list.extend([i.item() for i in list(y.cpu())]); pred_list.extend([i.item() for i in list(pred.cpu())])\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            # correct += torch.eq((pred>0).type(torch.int8), y).sum().item()\n",
    "            correct += torch.eq((pred>=0.5).type(torch.int8), y).sum().item()\n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation: \\n Accuracy: {correct:>0.5f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "\n",
    "    return valid_loss, correct, y_list, pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function: ensemble(MAE + pyradiomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_en(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    pred_list, y_list = [], []\n",
    "    train_running_loss = 0\n",
    "    train_running_correct = 0\n",
    "    for batch_data, (X, y) in dataloader:\n",
    "        '''\n",
    "        - batch_data: MRI images (4 modality)\n",
    "        - X, y: pyradiomics feature \n",
    "        '''\n",
    "        # for i in range(batch_size):\n",
    "        flairs, t1s, t1ces, t2s = batch_data[0]['flair'].to(device), batch_data[0]['t1'].to(device), \\\n",
    "            batch_data[0]['t1ce'].to(device), batch_data[0]['t2'].to(device)\n",
    "        X_mae = torch.cat((flairs, t1s, t1ces, t2s),dim=1)\n",
    "        \n",
    "        X_py, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 예측 오류 계산\n",
    "        pred = model(X_mae, X_py) # forward pass\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "        y_list.extend([i.item() for i in list(y.cpu())]); pred_list.extend([i.item() for i in list(pred.cpu())])\n",
    "        \n",
    "        loss = loss_fn(pred, y) # calculate the loss\n",
    "        train_running_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        # y_pred = (pred>0).type(torch.int8)\n",
    "        y_pred = (pred>=0.5).type(torch.int8)\n",
    "        train_running_correct += torch.eq(y_pred, y).sum().item()\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = train_running_loss / len(dataloader)\n",
    "    epoch_acc = train_running_correct / size\n",
    "    print(f\"Train: \\n Accuracy: {epoch_acc:>0.5f}%, Avg loss: {epoch_loss:>7f}\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, y_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_en(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    pred_list, y_list = [], []\n",
    "    valid_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # no training\n",
    "        for batch_data, (X, y) in dataloader:\n",
    "            '''\n",
    "            - batch_data: MRI images (4 modality)\n",
    "            - X, y: pyradiomics feature \n",
    "            '''\n",
    "            # for i in range(batch_size):\n",
    "            flairs, t1s, t1ces, t2s = batch_data[0]['flair'].to(device), batch_data[0]['t1'].to(device), \\\n",
    "                batch_data[0]['t1ce'].to(device), batch_data[0]['t2'].to(device)\n",
    "            X_mae = torch.cat((flairs, t1s, t1ces, t2s),dim=1)\n",
    "            \n",
    "            X_py, y = X.to(device), y.to(device)\n",
    "            pred = model(X_mae, X_py)\n",
    "            pred = nn.Sigmoid()(pred)\n",
    "            y_list.extend([i.item() for i in list(y.cpu())]); pred_list.extend([i.item() for i in list(pred.cpu())])\n",
    "            \n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            # correct += torch.eq((pred>0).type(torch.int8), y).sum().item()\n",
    "            correct += torch.eq((pred>=0.5).type(torch.int8), y).sum().item()\n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation: \\n Accuracy: {correct:>0.5f}%, Avg loss: {valid_loss:>8f} \\n\")\n",
    "\n",
    "    return valid_loss, correct, y_list, pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss_values, val_loss_values = [], []\n",
    "train_acc, val_acc = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load(os.path.join(root_dir, \"best_model.pth\"), map_location=device)\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(epochs):\n",
    "    print(f\"Epoch {ep+1}\\n-------------------------------\")\n",
    "    if mode == 'MAE':\n",
    "        train_epoch_loss, train_epoch_acc, y_list, pred_list = train_MAE(train_loader, model, loss_function, optimizer)\n",
    "        val_epoch_loss, val_epoch_acc, val_y_list, val_pred_list = valid_MAE(val_loader, model, loss_function)\n",
    "    elif mode == 'pyradiomics':\n",
    "        train_epoch_loss, train_epoch_acc, y_list, pred_list = train_py(train_loader_py, model, loss_function, optimizer)\n",
    "        val_epoch_loss, val_epoch_acc, val_y_list, val_pred_list = valid_py(val_loader_py, model, loss_function)\n",
    "    else:\n",
    "        train_epoch_loss, train_epoch_acc, y_list, pred_list = train_en(stack_train_loader, model, loss_function, optimizer)\n",
    "        val_epoch_loss, val_epoch_acc, val_y_list, val_pred_list = valid_en(stack_val_loader, model, loss_function)\n",
    "    \n",
    "    if len(val_loss_values)==0 or val_epoch_loss < min(val_loss_values):\n",
    "        torch.save(model.state_dict(), os.path.join(root_dir,f\"best_model.pth\"))\n",
    "        print(\"Saved Best Model State to model.pth\")\n",
    "        \n",
    "        best_y_list = copy.deepcopy(val_y_list)\n",
    "        best_pred_list = copy.deepcopy(val_pred_list)\n",
    "    epoch_loss_values.append(train_epoch_loss)  ;train_acc.append(train_epoch_acc)\n",
    "    val_loss_values.append(val_epoch_loss)      ;val_acc.append(val_epoch_acc)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(root_dir,f\"last_model.pth\"))\n",
    "print(\"Saved Last Model State to model.pth\")\n",
    "print(f\"Train Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx = torch.argmin (torch.tensor(val_loss_values))\n",
    "max_idx = torch.argmax(torch.tensor(val_acc))\n",
    "\n",
    "print(f\"Minimum validation loss is {val_loss_values[min_idx]:.5} in epoch {min_idx}\",\n",
    "      f\"\\nMaximum validation loss is {val_acc[max_idx]:.8} in epoch {max_idx}\")\n",
    "\n",
    "f= open(os.path.join(root_dir,\"result_summary.txt\"),\"w\")\n",
    "f.write(f\"Minimum validation loss is {val_loss_values[min_idx]:.5} in epoch {min_idx}\\n\")\n",
    "f.write(f\"Maximum validation loss is {val_acc[max_idx]:.8} in epoch {max_idx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'epoch':range(len(epoch_loss_values)),\n",
    "                   'train_loss':epoch_loss_values,  'val_loss':val_loss_values,\n",
    "                   'train_acc':train_acc,           'val_acc':val_acc})\n",
    "df = df.set_index('epoch')\n",
    "df.to_csv(os.path.join(root_dir,'save_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = val_acc\n",
    "fig = plt.figure(\"train/valid\", (18, 12))\n",
    "\n",
    "fig.add_subplot(2, 2, 1)\n",
    "plt.title('Iteration Train Loss')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(epoch_loss_values, label='train loss')\n",
    "\n",
    "fig.add_subplot(2, 2, 2)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [(i + 1) for i in range(len(val_loss_values))]\n",
    "# y = epoch_loss_values\n",
    "y = [epoch_loss_values[i-1] for i in x]\n",
    "y2 = [x for x in val_loss_values]\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y, label='train loss')\n",
    "plt.plot(x, y2, label='validation loss')\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(2, 2, 3)\n",
    "plt.title(\"Val Metric\")\n",
    "x = [(i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x,y)\n",
    "\n",
    "fig.add_subplot(2, 2, 4)\n",
    "plt.title(\"Val Metric\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "devide_num = int(len(y)*0.2)\n",
    "if devide_num==0: devide_num=1\n",
    "x2 = range(0,len(metric_values),devide_num)\n",
    "y2 = [metric_values[i] for i in x2]\n",
    "plt.bar(x2,y2,color=palette[:len(x)], width=devide_num*0.75)\n",
    "plt.savefig(os.path.join(root_dir,f'results_graph_{expd}.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Confusion matrix\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pred_label = [float(i>=0.5) for i in best_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "print(f\"validation AUC :  {roc_auc_score(y_true=best_y_list, y_score=best_pred_list)}\")\n",
    "\n",
    "f.write(f\"\\nvalidation AUC :  {roc_auc_score(y_true=best_y_list, y_score=best_pred_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "_, ax = plt.subplots(figsize=(7,7))\n",
    "disp = RocCurveDisplay.from_predictions(y_true=best_y_list, y_pred=best_pred_list, ax=ax) # display_labels=classes, --> 이미 actual, predicted가 라벨명으로 표현되어있어서 따로 주지 않아도 됨.\n",
    "disp.ax_.set_title(f'ROC curve ({mode})')\n",
    "ax.plot([0, 1], [0, 1], color='#FE5A6D', label='Random Model')\n",
    "ax.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(root_dir,f'ROC curve(Validation).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=best_y_list, y_pred=best_pred_label)\n",
    "\n",
    " # Sensitivity\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "# Specificity\n",
    "specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "print(f\"validation Sensitivity :  {sensitivity}\")\n",
    "print(f\"validation Specificity :  {specificity}\")\n",
    "\n",
    "f.write(f\"validation Sensitivity :  {sensitivity}\\n\")\n",
    "f.write(f\"validation Specificity :  {specificity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "_, ax = plt.subplots(figsize=(12,10))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_true=best_y_list, y_pred=best_pred_label, cmap=plt.cm.Blues, ax=ax, display_labels=le.classes_) # display_labels=classes, --> 이미 actual, predicted가 라벨명으로 표현되어있어서 따로 주지 않아도 됨.\n",
    "disp.ax_.set_title(f'Confusion matrix ({mode})')\n",
    "plt.savefig(os.path.join(root_dir,f'Confusion matrix(Validation).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyradiomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
